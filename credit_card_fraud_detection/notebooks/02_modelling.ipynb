{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5e77a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Add the project root (one level above /notebooks) to the very beginning of sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# 2️⃣ Import after the path has been updated\n",
    "from ccfd_utils.preprocessing import preprocess_data\n",
    "\n",
    "# 3️⃣ Load and preprocess the data\n",
    "df = pd.read_csv('../data/creditcard.csv')\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221ada6",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION\n",
    "\n",
    "To establish a baseline, a Logistic Regression classifier was trained on the preprocessed dataset using class weighting to address the imbalance between fraudulent and non-fraudulent transactions. The model achieved a high overall accuracy of ~98%, however this number is misleading due to the severe class imbalance. More importantly, it achieved a recall of 0.92 for the fraud class, meaning it was able to correctly detect most fraudulent transactions.\n",
    "On the other hand, the precision for the fraud class was only 0.06, indicating a large number of false positives.\n",
    "This makes Logistic Regression a good baseline for sensitivity (recall), but further models are necessary to improve precision without significantly reducing recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ee038e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56864\n",
      "           1       0.06      0.92      0.11        98\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.53      0.95      0.55     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n",
      "[[55478  1386]\n",
      " [    8    90]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression:\\n\", classification_report(y_test, lr_pred))\n",
    "print(confusion_matrix(y_test, lr_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33516a",
   "metadata": {},
   "source": [
    "DECISION TREES\n",
    "The Decision Tree model significantly improved performance compared to the baseline Logistic Regression. It achieved an overall accuracy of ~100% and, more importantly, much better performance on the fraud class.\n",
    "It reached a precision of 0.72 and a recall of 0.71 for fraudulent transactions, meaning it correctly captured the majority of frauds while greatly reducing false positives.\n",
    "This indicates that the model is able to learn non-linear relationships that the linear Logistic Regression model could not capture.\n",
    "However, decision trees can overfit easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d804d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.72      0.71      0.72        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.86      0.86      0.86     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "[[56837    27]\n",
      " [   28    70]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Decision Tree Model\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced')\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "print(\"Decision Tree:\\n\", classification_report(y_test, dt_pred))\n",
    "print(confusion_matrix(y_test, dt_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c63d5",
   "metadata": {},
   "source": [
    "RANDOM FOREST\n",
    "\n",
    "The Random Forest model achieved the best performance so far.\n",
    "It reached an overall accuracy close to 100%, with a precision of 0.96 and recall of 0.74 for the fraud class.\n",
    "This means that while the model still misses a few fraudulent transactions, the majority of the detected frauds are actually correct (very few false positives).\n",
    "\n",
    "Because Random Forest is an ensemble of many decision trees, it is able to capture complex, non-linear patterns in the data that single models (such as Logistic Regression or a single Decision Tree) could not.\n",
    "Overall, it offers a strong balance between recall and precision, which is important in fraud detection scenarios.\n",
    "\n",
    "Note: The n_jobs=-1 parameter tells the Random Forest to use all available CPU cores, which speeds up training significantly compared to using a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0987a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.96      0.74      0.84        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.98      0.87      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "[[56861     3]\n",
      " [   25    73]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,          # use all available cores .Enable parallel processing (uses multiple cores):\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print(\"Random Forest:\\n\", classification_report(y_test, rf_pred))\n",
    "print(confusion_matrix(y_test, rf_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f1237",
   "metadata": {},
   "source": [
    "XGBOOST \n",
    "\n",
    "XGBoost delivered the best overall performance among all models.\n",
    "It achieved 0.84 recall and 0.89 precision for the fraud class, meaning that the majority of fraudulent transactions were detected and most of the fraud predictions were correct.\n",
    "This indicates a very strong balance between catching frauds and keeping false alarms low.\n",
    "\n",
    "XGBoost works by building trees sequentially, each one correcting the errors of the previous tree.\n",
    "Because of this boosting approach, it is able to capture subtle patterns in the data that the other models may miss.\n",
    "For problems like credit card fraud detection—where fraud patterns are rare and complex—XGBoost often performs better than single models or bagging methods like Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "316071da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shail\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:06:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.89      0.84      0.86        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.95      0.92      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "[[56854    10]\n",
      " [   16    82]]\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "from xgboost import XGBClassifier\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    scale_pos_weight=len(y_train) / sum(y_train == 1),  # Adjust for class imbalance\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "print(\"XGBoost:\\n\", classification_report(y_test, xgb_pred))\n",
    "print(confusion_matrix(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c82f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved ✅\n"
     ]
    }
   ],
   "source": [
    "# --- after all evaluations ---\n",
    "\n",
    "# Save the two best models\n",
    "import joblib\n",
    "\n",
    "joblib.dump(rf_model,  \"../models/random_forest.pkl\")\n",
    "joblib.dump(xgb_model, \"../models/xgboost_best.pkl\")\n",
    "\n",
    "print(\"Models saved ✅\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
